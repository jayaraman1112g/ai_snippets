{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNNkVGK+0M5/X/P5Lk9yrHf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayaraman1112g/ai_snippets/blob/main/llamainde_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B7ofXdZE_ZBc"
      },
      "outputs": [],
      "source": [
        "!pip install -qU llama-index\n",
        "!pip install -qU transformers\n",
        "!pip install -qU sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU llama-index-embeddings-huggingface\n",
        "%pip install -qU llama-index-llms-huggingface"
      ],
      "metadata": {
        "id": "KeGD5w3c_cHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Read the document"
      ],
      "metadata": {
        "id": "QMTSF1kIWz5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF']='expandable_segments:True'"
      ],
      "metadata": {
        "id": "ULsxNfumd0Fj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
        "from llama_index.core import Settings\n",
        "\n",
        "\n",
        "documents = SimpleDirectoryReader(\n",
        "    input_files=[\"sample_data/DISEASE.pdf\"]\n",
        ").load_data()"
      ],
      "metadata": {
        "id": "uw_FRrN7IYjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the LLM"
      ],
      "metadata": {
        "id": "pTAd9r_cW6i_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are a Q&A assistant. Your goal is to answer questions as\n",
        "accurately as possible based on the instructions and context provided.\n",
        "\"\"\"\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "llm = HuggingFaceLLM(\n",
        "    context_window=4096,\n",
        "    max_new_tokens=256,\n",
        "    generate_kwargs={\"temperature\": 0.0, \"do_sample\": False},\n",
        "    system_prompt=system_prompt,\n",
        "    tokenizer_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    model_name=\"meta-llama/Llama-2-7b-chat-hf\",\n",
        "    device_map=\"auto\",\n",
        "    # loading model in 8bit for reducing memory\n",
        "    model_kwargs={\"torch_dtype\": torch.float16 }\n",
        ")"
      ],
      "metadata": {
        "id": "xjocYlxSIj_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup the embedding"
      ],
      "metadata": {
        "id": "mZybE6Kh224a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "embed_model = HuggingFaceEmbedding (model_name='BAAI/bge-small-en-v1.5',token=hf_token )"
      ],
      "metadata": {
        "id": "gqGpvvYKMjwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4QOfYzbSIY0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# get index from VectorStoreIndex"
      ],
      "metadata": {
        "id": "BPXxCvBU29Qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core import ServiceContext\n",
        "service_context=ServiceContext.from_defaults(\n",
        "    chunk_size=1024,\n",
        "    llm=llm,\n",
        "    embed_model=embed_model\n",
        ")\n",
        "index = VectorStoreIndex.from_documents (documents, service_context = service_context)"
      ],
      "metadata": {
        "id": "cJX5UAcOZvWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Obtain and retriever and qe from Index"
      ],
      "metadata": {
        "id": "PckD0M3G3FB9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.response.notebook_utils import display_source_node\n",
        "retriever = index.as_retriever ()\n",
        "nodes = retriever.retrieve (\"What is diphtheria?\")\n",
        "for node in nodes:\n",
        "    display_source_node(node)"
      ],
      "metadata": {
        "id": "zwfNyRLMbodA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qe = index.as_query_engine ()\n",
        "nodes = qe.query (\"What is diptheria\")\n",
        "print (nodes)"
      ],
      "metadata": {
        "id": "AWTJKAgkczpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setup vector store"
      ],
      "metadata": {
        "id": "gKb-owJO6T1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "!pip install -qU chromadb"
      ],
      "metadata": {
        "id": "Xa7bI0Kl67qS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "client = chromadb.Client()\n",
        "db = client.get_or_create_collection(\"test_llamaindex\")\n",
        "print (len(documents))\n",
        "for i,doc in enumerate (documents) :\n",
        "  # print (i)\n",
        "  db.add (documents=[doc], ids=str(i))\n",
        "\n",
        "\n",
        "# pd.DataFrame(db.peek(0))"
      ],
      "metadata": {
        "id": "Bsk6uEuR7Si4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU llama-index-vector-stores-qdrant"
      ],
      "metadata": {
        "id": "LHq-Pna69nhb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
        "\n",
        "import qdrant_client\n",
        "\n",
        "client = qdrant_client.QdrantClient(location=\":memory:\")\n",
        "vector_store = QdrantVectorStore(client=client, collection_name=\"test_store\")"
      ],
      "metadata": {
        "id": "ijjsyWjd9kvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup ingestion pipeline"
      ],
      "metadata": {
        "id": "_o0g0SBh3I6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.ingestion import IngestionPipeline\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core.extractors import TitleExtractor\n",
        "\n",
        "pipeline = IngestionPipeline (\n",
        "            transformations=[\n",
        "                SentenceSplitter (chunk_size = 200, chunk_overlap=10),\n",
        "                TitleExtractor (llm=llm),\n",
        "                embed_model\n",
        "                # HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en-v1.5\")\n",
        "            ],\n",
        "            vector_store=vector_store\n",
        ")\n"
      ],
      "metadata": {
        "id": "VnF0M6p4xRCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "nest_asyncio.apply()\n",
        "nodes = pipeline.run(documents=documents)"
      ],
      "metadata": {
        "id": "UkCD_j3V-D64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index = VectorStoreIndex.from_vector_store (vector_store=vector_store, service_context=service_context)"
      ],
      "metadata": {
        "id": "jBJLCxFgFKvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "qe = index.as_query_engine ()\n",
        "print (qe.query (\"What is diptheria?\"))"
      ],
      "metadata": {
        "id": "GyYQ8P7FWQzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-JNYAMtcWW4W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}